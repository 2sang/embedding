---
layout: default
title: 버그 신고 및 정오표
description: 코드/도서 오류를 리포트하는 방법과 정정 결과를 안내합니다.
---



## 버그 리포트하기

꼼꼼히 확인한다고 했지만 책 내용이나 코드에 오류나 버그가 있을 수 있습니다. 이 경우 [이곳](https://github.com/ratsgo/embedding/issues/new)에 접속해 이슈(issue)를 작성하시면 됩니다.

이슈를 만드는 방법은 다음과 같습니다. 아래 그림의 양식을 채워 `Submit new issue` 버튼을 누르면 이슈가 완성됩니다.



<img src="https://i.imgur.com/9tVHsZ8.png" width="600px" title="source: imgur.com" /><



## 정정 결과

도서 및 코드의 주요 정정 결과를 안내합니다.

**P. 32~33**

```
구체적으로는 첫 번째 단어 벡터 + 두 번째 단어 벡터 - 세 번째 단어 벡터를 계산해 보는 것이다. 그림 1-3처럼 아들 + 딸 - 소녀 = 소년이 성립하면 성공적인 임베딩이라고 볼 수 있다. (중략) 단어1 + 단어2 - 단어3 연산을 수행한 벡터와 코사인 유사도가 가장 높은 단어들이 네 번째 열의 단어들이다.
>
구체적으로는 첫 번째 단어 벡터 - 두 번째 단어 벡터 + 세 번째 단어 벡터를 계산해 보는 것이다. 그림 1-3처럼 아들 - 딸 + 소녀 = 소년이 성립하면 성공적인 임베딩이라고 볼 수 있다. (중략) 단어1 - 단어2 + 단어3 연산을 수행한 벡터와 코사인 유사도가 가장 높은 단어들이 네 번째 열의 단어들이다.
```

**P. 72**

```
언어학자들이 제시하는 품사 분류 기준은 기능(function), 의미(meaning), 형태(form) 등 세 가지다. 
> 
학교문법에 따르면 품사 분류 기준은 기능(function), 의미(meaning), 형식(form) 등 세 가지다.
```

**P. 122**

```
수식 4-6에서 f(w_i)란 해당 단어가 말뭉치에서 차지하는 비율(해당 단어 빈도/어휘 집합 크기)을 의미한다.
>
수식 4-6에서 U(w_i)란 해당 단어의 유니 그램 확률(해당 단어 빈도/전체 단어 수)을 의미한다.
수식 4-6 : f(w_i)를 U(w_i)로 대체
```

**P. 123**

```
서브샘플링 확률은 수식 4-8과 같다. (중략) 만일 f(w_i)가 0.01로 나타나는 빈도 높은 단어(예컨대 조사 은/는)는 위 식으로 계산한 P(w_i)가 0.9684나 돼서 해당 단어가 가질 수 있는 100번의 학습 기회 가운데 3~4번 정도는 학습에서 제외하게 된다. 반대로 등장 비율이 적어 P(w_i)가 0에 가깝다면 해당 단어가 나올 때마다 빼놓지 않고 학습을 시키는 구조다.
>
서브샘플링 확률은 수식 4-8과 같다. f(w_i)는 w_i의 빈도를 가리키며 t는 하이퍼파라메터이다. Mikolov et al. (2013b)은 t를 10^{-5}로 설정했다. (중략) 만일 f(w_i)가 0.01로 나타나는 빈도 높은 단어(예컨대 조사 은/는)는 위 식으로 계산한 P_{subsampling}(w_i)가 0.9684나 돼서 해당 단어가 가질 수 있는 100번의 학습 기회 가운데 96번 정도는 학습에서 제외하게 된다. 반대로 등장 비율이 적어 P_{subsampling}(w_i)가 0에 가깝다면 해당 단어가 나올 때마다 빼놓지 않고 학습을 시키는 구조다.
 ```

**P. 222**

```
개별 소프트맥스 값이 지나치게 작아지는 것을 방지 
> 
소프트맥스의 그래디언트가 지나치게 작아지는 것을 방지
```

**P. 225~226**

```
Pointwise Feedforward Networks
>
Position-wise Feedforward Networks
```

**P. 230**

```
전체 학습 데이터 토큰의 15%를 마스킹한다 
> 
학습 데이터 한 문장 토큰의 15%를 마스킹한다
```